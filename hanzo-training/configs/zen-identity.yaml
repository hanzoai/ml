# Zen Identity Dataset Training Configuration
# Fine-tune model personality and identity after foundation training

model:
  name: "zen-coder-4b"
  architecture: "qwen"
  checkpoint: "./output/zen-coder-4b"  # Pre-trained on agentic data
  max_seq_length: 2048
  vocab_size: 151936
  hidden_size: 3584
  num_layers: 32
  num_heads: 28

dataset:
  name: "zen-identity"
  path: "/Users/z/work/zen/zen-identity-dataset"
  format: "jsonl"
  train_split: "train"
  validation_split: null
  max_seq_length: 2048
  preprocessing:
    tokenizer: "Qwen/Qwen3-4B-Instruct"
    add_eos: true
    add_bos: false
    truncation: true
    padding: "right"

training:
  # Identity fine-tuning parameters
  batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 0.0001  # 1e-4 (lower for fine-tuning)
  weight_decay: 0.005
  warmup_steps: 50
  max_steps: 5000
  epochs: 3
  
  # Optimization
  optimizer: "adamw"
  scheduler: "linear"
  gradient_clipping: 0.5
  
  # LoRA for identity fine-tuning
  lora:
    enabled: true
    r: 32  # Smaller rank for identity tuning
    alpha: 64.0
    dropout: 0.05
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
    bias: "none"
  
  # More frequent checkpointing for identity tuning
  save_steps: 500
  eval_steps: 250
  logging_steps: 5
  
  # Hardware optimization
  mixed_precision: "bf16"
  device: "cuda:0"
  multi_gpu: false
  compile: true

evaluation:
  benchmarks: ["accuracy"]
  metrics: ["loss", "accuracy"]
  output_dir: "./eval_results/identity"

logging:
  wandb:
    enabled: true
    project: "hanzo-training"
    name: "zen-identity-fine-tuning"
    tags: ["identity", "personality", "fine-tuning"]
    notes: "Fine-tuning model identity and personality on zen-identity-dataset"
  
  tensorboard: true
  console_level: "info"
  
  file_logging:
    enabled: true
    path: "./logs/identity_training.log"
    level: "debug"