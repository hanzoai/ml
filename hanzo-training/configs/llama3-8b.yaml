# Llama 3.1 8B Training Configuration
# Train Llama 3.1 8B on zen-agentic-dataset

model:
  name: "llama3-8b"
  architecture: "llama"
  checkpoint: "meta-llama/Meta-Llama-3.1-8B-Instruct"
  max_seq_length: 8192
  vocab_size: 128256
  hidden_size: 4096
  num_layers: 32
  num_heads: 32

dataset:
  name: "zen-agentic"
  path: "/Users/z/work/zen/zen-agentic-dataset-private"
  format: "jsonl"
  train_split: "train"
  validation_split: "valid"
  max_seq_length: 8192
  preprocessing:
    tokenizer: "meta-llama/Meta-Llama-3.1-8B-Instruct"
    add_eos: true
    add_bos: true  # Llama uses BOS
    truncation: true
    padding: "right"

training:
  # Parameters optimized for Llama 3.1 8B
  batch_size: 2  # Smaller batch for 8B model
  gradient_accumulation_steps: 4
  learning_rate: 0.0001  # 1e-4 (slightly lower for larger model)
  weight_decay: 0.01
  warmup_steps: 200
  max_steps: 15000
  epochs: 2
  
  # Optimization
  optimizer: "adamw"
  scheduler: "cosine"
  gradient_clipping: 1.0
  
  # LoRA for 8B model
  lora:
    enabled: true
    r: 32  # Smaller rank for larger model
    alpha: 64.0
    dropout: 0.1
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    bias: "none"
  
  # Checkpointing
  save_steps: 1500
  eval_steps: 750
  logging_steps: 10
  
  # Hardware requirements
  mixed_precision: "bf16"
  device: "cuda:0"
  multi_gpu: false
  compile: true

evaluation:
  benchmarks: ["perplexity", "accuracy"]
  metrics: ["loss", "accuracy", "perplexity"]
  output_dir: "./eval_results/llama8b"

logging:
  wandb:
    enabled: true
    project: "hanzo-training"
    name: "llama3-8b-zen-agentic"
    tags: ["llama", "agentic", "8b"]
    notes: "Training Llama 3.1 8B on zen-agentic-dataset"
  
  tensorboard: true
  console_level: "info"
  
  file_logging:
    enabled: true
    path: "./logs/llama8b_training.log"
    level: "debug"